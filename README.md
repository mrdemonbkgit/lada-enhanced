<h1 align="center">
  <img src="assets/io.github.ladaapp.lada.png" alt="Lada Icon" style="display: block; width: 64px; height: 64px;">
  <br>
  Lada
</h1>

*Lada* is a tool designed to recover pixelated adult videos (JAV). It helps restore the visual quality of such content, making it more enjoyable to watch.

## Features

- **Recover Pixelated Videos**: Restore pixelated or mosaic scenes in adult videos.
- **Watch/Export Videos**: Use either the CLI or GUI to watch or export your restored videos.
- **Multi-GPU Parallel Processing**: Split video across multiple GPUs for near-linear speedup (e.g. ~4x with 4 GPUs).

## Usage

### GUI

After opening a file, you can either watch the restored video in real time or export it to a new file to watch it later:

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="assets/screenshot_gui_1_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="assets/screenshot_gui_1_light.png">
  <img alt="Screenshot showing video preview" src="assets/screenshot_gui_1_dark.png" width="36%">
</picture>
<picture>
  <source media="(prefers-color-scheme: dark)" srcset="assets/screenshot_gui_2_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="assets/screenshot_gui_2_light.png">
  <img alt="Screenshot showing video export" src="assets/screenshot_gui_2_dark.png" width="45%">
</picture>

Additional settings can be found in the left sidebar.

### CLI

You can also use the command-line interface (CLI) to restore video(s):

```shell
lada-cli --input <input video path>
```
<img src="assets/screenshot_cli_1.png" alt="screenshot showing video export" width="60%">

#### Multi-GPU Processing

If you have multiple GPUs, you can process videos significantly faster by splitting the work across all available GPUs:

```shell
lada-cli --input <input video path> --num-gpus 4
```

Each GPU processes a segment of the video in parallel, then the segments are seamlessly merged. This provides near-linear speedup (e.g. a 55-minute video that takes ~5 minutes on 1 GPU can be processed in ~1.5 minutes on 4 GPUs).

| Option | Default | Description |
|--------|---------|-------------|
| `--num-gpus` | `1` | Number of CUDA GPUs to use for parallel processing |
| `--overlap-frames` | `60` | Number of overlap frames between segments to prevent boundary artifacts |

> [!TIP]
> For best quality with multi-GPU, use the accurate detection model and a high-quality encoding preset:
> ```shell
> lada-cli --input <input video path> --num-gpus 4 --mosaic-detection-model v4-accurate --encoding-preset hevc-nvidia-gpu-uhq --max-clip-length 360
> ```

For more information about additional options, use the `--help` argument.

## Performance expectations and hardware requirements
The restoration quality can vary depending on the scene. Some may look quite realistic, while others could display noticeable artifacts, sometimes worse than the original mosaics.

To run the app effectively, you’ll need a GPU and some patience. A graphics card with at least 4-6GB of VRAM should work well for most cases.

The app also requires a fair amount of RAM for buffering, which improves performance. For 1080p content, 6-8GB of RAM should suffice, but 4K video will require significantly more.

To watch the restored video in real-time, you’ll need a powerful machine. Otherwise, the player may pause and buffer as it computes the next set of restored frames. While viewing the video, no encoding is done, but additional RAM will be used for buffering.

If your GPU isn’t fast enough for real-time playback, you can export the video and watch it later in your preferred media player (this is supported in both the GUI and CLI).

**Multi-GPU systems**: If you have multiple NVIDIA GPUs, you can use `--num-gpus` in the CLI to split the video across all GPUs for near-linear speedup. Each GPU loads its own models and processes a segment independently, so VRAM requirements remain the same per GPU.

Although the app can run on a CPU, performance will be extremely slow, making it impractical for most users.

## Installation
### Using Flatpak
The easiest way to install the app (CLI and GUI) on Linux is via Flathub:

<a href='https://flathub.org/apps/details/io.github.ladaapp.lada'><img width='200' alt='Download from Flathub' src='https://flathub.org/api/badge?svg&locale=en'/></a>

> [!NOTE]
> The Flatpak only works with x86_64 CPUs. Nvidia/CUDA (Turing or newer: RTX 20xx up to including RTX 50xx) and Intel Arc GPUs are supported. Ensure your GPU driver is up-to-date.
> It can also be used without a GPU but it will be very slow. Make sure to install either the Intel or the Nvidia Add-On from Flathub.

> [!TIP]
> After installation you should find Lada in your application launcher to start the GUI. You can also run it via `flatpak run io.github.ladaapp.lada`.

> [!TIP]
> When using the CLI via Flatpak we need to make the file/directory available by giving it permission to the file system so it can access the input/output files
>  ```shell
>  flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada --input <input video path>
>  ```
> You may want to set an alias to make it easier to use
> ```shell
> alias lada-cli="flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada"
>  ```
> You could also give the filesystem permission permanently via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) 

> [!TIP]
> If you want to use the Post-export action feature to run a command/script after export has finished you'll need to give the Flatpak additional permissions.
> Add the `--talk-name=org.freedesktop.Flatpak` permission and then run your command via `flatpak-spawn`. For example: If the script you want to run is /home/user/myscript.sh then set custom command as `flatpak-spawn --host /home/user/myscript.sh`

> [!TIP]
> If you installed Lada from Flathub and drag-and-drop doesn't work, your file browser might not support [File Transfer Portal](https://flatpak.github.io/xdg-desktop-portal/docs/doc-org.freedesktop.portal.FileTransfer.html).
> You can fix this by:
>  1) Switching or updating your file browser to one that supports it.
>  2) Granting the app filesystem permissions (e.g., via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) so it can read files directly).
>  3)  Using the 'Open' button to select the file instead of drag-and-drop.

### Using Docker

The app is also available via Docker (CLI only). You can get the image `ladaapp/lada` from [Docker Hub](https://hub.docker.com/r/ladaapp/lada) with this command:

```shell
docker pull ladaapp/lada:latest
````

> [!NOTE]
> The Docker image only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
> It can also be used without a GPU but it will be very slow.

> [!TIP]
> Make sure that you have installed the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) on your system so Docker can pass through the GPU

> [!TIP]
> When using Docker you'll need to make the file/directory available to the container as well as the GPU:
>  ```shell
> docker run --rm --gpus all --mount type=bind,src=<input video path>,dst=/mnt ladaapp/lada:latest --input "/mnt/<input video file>"
> ```
> For multi-GPU processing, pass all GPUs and use `--num-gpus`:
>  ```shell
> docker run --rm --gpus all --mount type=bind,src=<input video path>,dst=/mnt ladaapp/lada:latest --input "/mnt/<input video file>" --num-gpus 4
> ```

> [!TIP]
> If you want to use hardware encoders like `hevc_nvenc` you have to provide the container with `video` capability.
> 
> With docker run you can use `--gpus 'all,"capabilities=compute,video"'`. Learn more [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html).

### Using Windows

For Windows users, the app (CLI and GUI) is packaged as a standalone .7z archive file.
You'll need [7-zip](https://7-zip.org/) to unpack the files. It is recommended to validate the file after downloading. See the Tip below.

Get the latest release from the [Releases Page](https://codeberg.org/ladaapp/lada/releases).

You'll find `lada.exe` and `lada-cli.exe` after extracting the archive.

> [!NOTE]
> The Windows release only works with x86_64 CPUs. Nvidia/CUDA (Turing or newer: RTX 20xx up to including RTX 50xx) and Intel Arc GPUs are supported. Ensure your GPU driver is up-to-date.
> It can also be used without a GPU but it will be very slow.

> [!NOTE]
> Be aware that the first start of lada.exe or lada-cli.exe could take a while before Windows Defender or your AV has scanned it. The next time you open the program it should start fast.

> [!TIP]
> It is recommended to compare the checksum of the downloaded file against the value you'll find in the release announcement.
> This makes sure that you got the correct and unaltered file, especially important if you got the file from an unofficial source.
> 
> Calculate the checksum of the downloaded file on your computer and compare it against the `SHA256` value you'll find in the release announcement. They must be the same!
> 
> You can do this with Powershell `Get-FileHash /path/to/file.7z` or [QuickHash-GUI](https://www.quickhash-gui.org/).

### Alternative Installation Methods

If the packages above don't work for you then you'll have to follow the [Build](#build) steps to set up the project.

Note that these instructions are mostly intended for developers to set up their environment to start working on the source code. But you should hopefully be able
to follow the instructions even if you aren't a developer.

Officially, Lada supports only Nvidia and Intel Arc GPUs but there have been reports that AMD ROCm-compatible cards and Apple work as well.

You can check the issue tracker to find out more about the current state of supporting other systems.

## Contribute

You can find the Lada project [on GitHub](https://github.com/ladaapp/lada) and [on Codeberg](https://codeberg.org/ladaapp/lada).

The home of the project is on Codeberg. GitHub is set up only as a mirror so it's code will stay in sync with the main branch on Codeberg.

For contributing code, ideas or bug reports use [Pull requests](https://codeberg.org/ladaapp/lada/pulls) and the [Issue tracker](https://codeberg.org/ladaapp/lada/issues) on Codeberg.

If you want to help translating the app you can contribute to existing translations or set up a new language over at [Codeberg Translate](https://translate.codeberg.org/projects/lada/lada/).

[![Translation status](https://translate.codeberg.org/widget/lada/lada/multi-auto.svg)](https://translate.codeberg.org/engage/lada/)

## Releases

New releases will be published on both [GitHub Releases](https://github.com/ladaapp/lada/releases) and [Codeberg Releases](https://codeberg.org/ladaapp/lada/releases). You should get a notification about new releases if you star the project on either platform.

## Build

If you want to start hacking on this project you'll need to install the app from source. Check out the detailed installation guides for [Linux](docs/linux_install.md) and [Windows](docs/windows_install.md).

## Training and dataset creation

For instructions on training your own models and datasets, refer to [Training and dataset creation](docs/training_and_dataset_creation.md).

## License

Source code and models are licensed under AGPL-3.0. See the [LICENSE.md](LICENSE.md) file for full details.

## Acknowledgement
This project builds upon work done by these fantastic individuals and projects:

* [DeepMosaics](https://github.com/HypoX64/DeepMosaics): Provided code for mosaic dataset creation. Also inspired me to start this project.
* [BasicVSR++](https://ckkelvinchan.github.io/projects/BasicVSR++) / [MMagic](https://github.com/open-mmlab/mmagic): Used as the base model for mosaic removal.
* [YOLO/Ultralytics](https://github.com/ultralytics/ultralytics): Used for training mosaic and NSFW detection models.
* [DOVER](https://github.com/VQAssessment/DOVER):  Used to assess video quality of created clips during the dataset creation process to filter out low-quality clips.
* [DNN Watermark / PITA Dataset](https://github.com/tgenlis83/dnn-watermark): Used most of its code for creating a watermark detection dataset used to filter out scenes obstructed by text/watermarks/logos.
* [NudeNet](https://github.com/notAI-tech/NudeNet/): Used as an additional NSFW classifier to filter out false positives by our own NSFW segmentation model
* [Twitter Emoji](https://github.com/twitter/twemoji): Provided eggplant emoji as base for the app icon.
* [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): Used their image degradation model design for our mosaic detection model degradation pipeline.
* [BPJDet](https://github.com/hnuzhy/BPJDet): Model for detecting human body and head. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* [CenterFace](https://github.com/Star-Clouds/CenterFace): Model for detecting human faces. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* PyTorch, FFmpeg, GStreamer, GTK and [all other folks building our ecosystem](https://xkcd.com/2347/)
